# Benchmarking HRL Models and Algorithms

TODO

## Contents

* [Running Existing Models and Algorithms](#running-existing-models-and-algorithms)
* [Visualizing Pre-trained Results](#visualizing-pre-trained-results)
* [Benchmarked Results](#benchmarked-results)

## Running Existing Models and Algorithms

TODO (@chendi)

## Visualizing Pre-trained Results

TODO (@aboudy)

## Benchmarked Results

Several variants of the above models and algorithms have been rested and 
benchmarked on a suite of continuous control tasks described in the 
[TODO](TODO) section of the repository's main README file. The results can be 
downloaded by running the following script:

```shell script
TODO
```
Note that, since the data is extensive, parts of it can be downloaded 
separately. This can be done via the following command-line arguments:

* TODO

For example, if you would like to determine which files are currently 
accessible, type:

```shell script
TODO
```

Moreover, if you would like to install a subset of these files, say TODO and 
TODO, type:

```shell script
TODO
```

The results can be visualized analyzed via a supplementary jupyter notebook 
file located within this directory titled [TODO](TODO). You are encouraged to 
look through this file to understand more about the collected results.
